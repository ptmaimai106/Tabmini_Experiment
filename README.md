# Chiáº¿n lÆ°á»£c tiáº¿p cáº­n

## ğŸ“Œ 1. Vá»›i mÃ´ hÃ¬nh XGBoost vÃ  LightGBM (GBDT)
### ğŸ‘‰ PhÆ°Æ¡ng phÃ¡p phÃ¹ há»£p:
- DÃ¹ng `GridSearchCV` hoáº·c `RandomizedSearchCV` cá»§a Scikit-Learn Ä‘á»ƒ tÃ¬m tham sá»‘ tá»‘i Æ°u.
- CÃ³ thá»ƒ sá»­ dá»¥ng `train_test_split` Ä‘á»ƒ chia táº­p dá»¯ liá»‡u.
- KhÃ´ng cáº§n train nhiá»u epoch vÃ¬ mÃ´ hÃ¬nh Boosting tá»± há»c dá»±a trÃªn decision tree.

### ğŸ‘‰ LÃ½ do chá»n GridSearchCV:
- CÃ¡c mÃ´ hÃ¬nh Boosting cÃ³ Ã­t tham sá»‘ hÆ¡n Deep Learning nhÆ°ng ráº¥t nháº¡y vá»›i tham sá»‘ nhÆ° `learning_rate`, `max_depth`, `n_estimators`.
- `GridSearchCV` giÃºp thá»­ nhiá»u bá»™ tham sá»‘ vÃ  tÃ¬m ra bá»™ tá»‘t nháº¥t báº±ng cross-validation.

### Training and result
- Ref: [define and train model here](https://github.com/ptmaimai106/Tabmini_Experiment/blob/main/boosting-models.py)
- Output model: saved_models/xgboost vÃ  saved_models/lightgbm
- Result: results/boosting

---

## ğŸ“Œ 2. Vá»›i MLP-PLR (Neural Network)
### ğŸ‘‰ PhÆ°Æ¡ng phÃ¡p phÃ¹ há»£p:
- ÄÃ¢y lÃ  mÃ´ hÃ¬nh deep learning nÃªn cáº§n dÃ¹ng **PyTorch** hoáº·c **TensorFlow**.
- Huáº¥n luyá»‡n báº±ng optimizer (`SGD`, `Adam`, `RMSprop`).
- Cáº§n nhiá»u epoch (vÃ­ dá»¥: `50â€“100`) vÃ  dÃ¹ng **early stopping** Ä‘á»ƒ trÃ¡nh overfitting.

### Training and result
- Ref: [define and train model here](https://github.com/ptmaimai106/Tabmini_Experiment/blob/main/train_07_MLP_PLR.py)
- Output model: saved_models/mlp
- Result: results/mlp
Note: dá»±a vÃ o config cá»§a boosting models, Ä‘á»ƒ so sÃ¡nh trá»±c quan cho cÃ¡c táº­p dataset nÃ y thÃ¬ thá»­ nghiá»‡m MLP vá»›i sá»‘ epoch lÃ  500
---

## ğŸ“Œ 3. Vá»›i TabTransformer & SAINT (Transformer-based models)
### ğŸ‘‰ PhÆ°Æ¡ng phÃ¡p phÃ¹ há»£p:
- **TabTransformer** vÃ  **SAINT** lÃ  mÃ´ hÃ¬nh dá»±a trÃªn **Transformer** â†’ cáº§n huáº¥n luyá»‡n giá»‘ng nhÆ° mÃ´ hÃ¬nh deep learning.
- Cáº§n huáº¥n luyá»‡n nhiá»u epoch, cÃ³ thá»ƒ dÃ¹ng optimizer nhÆ° `Adam`.
- Cáº§n sá»­ dá»¥ng ká»¹ thuáº­t nhÆ° **learning rate decay**, **dropout**, **batch normalization** Ä‘á»ƒ trÃ¡nh overfitting.
- ### Training and result
- Ref: [define and train model here](https://github.com/ptmaimai106/Tabmini_Experiment/blob/main/train_07_MLP_PLR.py)
- Output model: saved_models/mlp
- Result: results/mlp
Note: dá»±a vÃ o config cá»§a boosting models, Ä‘á»ƒ so sÃ¡nh trá»±c quan cho cÃ¡c táº­p dataset nÃ y thÃ¬ thá»­ nghiá»‡m MLP vá»›i sá»‘ epoch lÃ  500


# Deprecated dataset
Nhá»¯ng dataset sau Ä‘Ã¢y bá»‹ loáº¡i bá» ra khá»i pmlb vÃ o ngÃ y 20/02/2025 nÃªn khÃ´ng Ä‘Æ°á»£c training.
Exclude manual táº¡i Ä‘Ã¢y: TabMini/tabmini/data/data_info.py

"cleve",
"horse_colic",
"vote",
"heart_c",
"house_votes_84",
"colic",
"heart_h",
"heart_statlog",
"hungarian"

